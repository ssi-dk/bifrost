import re
import pandas
from ruamel.yaml import YAML
import sys

# from pytools.persistent_dict import PersistentDict
# size estimation from kmers as well with jellyfish?
# storage = PersistentDict("qcquickie_storage")
# alternative is to do a quick qc on basepairs followed by a more rigorous assembly

configfile: os.path.join(os.path.dirname(workflow.snakefile), "../config.yaml")
# requires --config R1_reads={read_location},R2_reads={read_location}
sample = config["Sample"]
R1 = config["R1_reads"]
R2 = config["R2_reads"]
global_threads = config["global"]["threads"]
global_memory_in_GB = config["global"]["memory"]

yaml = YAML(typ='safe')
yaml.default_flow_style = False
with open(sample, "r") as yaml_stream:
    config_sample = yaml.load(yaml_stream)

folder_name = "qcquickie"
# my understanding is all helps specify final output
onsuccess:
    print("Workflow complete")
    config_sample["sample"]["status"] = "qcquickie_completed_successfully"
    with open(sample, "w") as output_file:
        yaml.dump(config_sample, output_file)
    shell("rm contigs.cov contigs.sam filtered.fastq merged.fastq unmerged.fastq")

onerror:
    print("Workflow error")
    config_sample["sample"]["status"] = "qcquickie_failed"
    with open(sample, "w") as output_file:
        yaml.dump(config_sample, output_file)


rule all:
    input:
        folder_name,
        os.path.join(folder_name, "contigs.bin.cov"),
        os.path.join(folder_name, "species.txt"),
        os.path.join(folder_name, "contaminantion_check.txt")


rule setup:
    output:
        dir = folder_name
    shell:
        "mkdir {output}"


rule setup__filter_reads_with_bbduk:
    message:
        "Running step: {rule}"
    input:
        dir = folder_name,
        reads = (R1, R2)
    output:
        filtered_reads = os.path.join(folder_name, "filtered.fastq")
    params:
        adapters = os.path.join(os.path.dirname(workflow.snakefile), "../resources/adapters.fasta")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/setup__filter_reads_with_bbduk.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/setup__filter_reads_with_bbduk.benchmark")
    shell:
        "bbduk.sh threads={threads} -Xmx{resources.memory_in_GB}G in={input.reads[0]} in2={input.reads[1]} out={output.filtered_reads} ref={params.adapters} ktrim=r k=23 mink=11 hdist=1 tbo minavgquality=14 &> {log}"


rule contaminant_check__classify_reads_kraken_minikraken_db:
    message:
        "Running step: {rule}"
    input:
        filtered_reads = os.path.join(folder_name, "filtered.fastq")
    output:
        kraken_report = os.path.join(folder_name, "kraken_report.txt")
    params:
        db = "/srv/data/DB/kraken/minikraken_20171019_8GB/"
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/contaminant_check__classify_reads_kraken_minikraken_db.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/contaminant_check__classify_reads_kraken_minikraken_db.benchmark")
    shell:
        "kraken --threads {threads} -db {params.db} --fastq-input {input.filtered_reads} 2> {log} | kraken-report -db {params.db} 1> {output.kraken_report}"


rule contaminant_check__determine_species_bracken_on_minikraken_results:
    message:
        "Running step: {rule}"
    input:
        kraken_report = os.path.join(folder_name, "kraken_report.txt")
    output:
        bracken = os.path.join(folder_name, "bracken.txt")
    params:
        kmer_dist = "/srv/data/DB/kraken/minikraken_20171019_8GB/minikraken_8GB_100mers_distrib.txt"
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/contaminant_check__determine_species_bracken_on_minikraken_results.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/contaminant_check__determine_species_bracken_on_minikraken_results.benchmark")
    shell:
        """
        est_abundance.py -i {input.kraken_report} -k {params.kmer_dist} -o {output.bracken} &> {log}
        sort -r -t$'\t' -k7 {output.bracken} -o {output.bracken}
        """


rule assembly_check__combine_reads_with_bbmerge:
    message:
        "Running step: {rule}"
    input:
        filtered_reads = os.path.join(folder_name, "filtered.fastq")
    output:
        merged_reads = os.path.join(folder_name, "merged.fastq"),
        unmerged_reads = os.path.join(folder_name, "unmerged.fastq")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/assembly_check__combine_reads_with_bbmerge.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/assembly_check__combine_reads_with_bbmerge.benchmark")
    shell:
        "bbmerge.sh threads={threads} -Xmx{resources.memory_in_GB}G in={input.filtered_reads} out={output.merged_reads} outu={output.unmerged_reads} &> {log}"


rule assembly_check__quick_assembly_with_tadpole:
    message:
        "Running step: {rule}"
    input:
        merged_reads = os.path.join(folder_name, "merged.fastq"),
        unmerged_reads = os.path.join(folder_name, "unmerged.fastq")
    output:
        contigs = os.path.join(folder_name, "contigs.fasta")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/assembly_check__quick_assembly_with_tadpole.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/assembly_check__quick_assembly_with_tadpole.benchmark")
    shell:
        "tadpole.sh threads={threads} -Xmx{resources.memory_in_GB}G in={input.merged_reads},{input.unmerged_reads} out={output.contigs} &> {log}"


rule assembly_check__map_reads_to_assembly_with_bbwrap:
    message:
        "Running step: {rule}"
    input:
        contigs = os.path.join(folder_name, "contigs.fasta"),
        merged_reads = os.path.join(folder_name, "merged.fastq"),
        unmerged_reads = os.path.join(folder_name, "unmerged.fastq")
    output:
        mapped = os.path.join(folder_name, "contigs.sam")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/assembly_check__map_reads_to_assembly_with_bbwrap.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/assembly_check__map_reads_to_assembly_with_bbwrap.benchmark")
    shell:
        "bbwrap.sh threads={threads} -Xmx{resources.memory_in_GB}G ref={input.contigs} in={input.merged_reads},{input.unmerged_reads} out={output.mapped} append &> {log}"


rule assembly_check__pileup_on_mapped_reads:
    message:
        "Running step: {rule}"
    input:
        mapped = os.path.join(folder_name, "contigs.sam")
    output:
        coverage = os.path.join(folder_name, "contigs.cov"),
        pileup = os.path.join(folder_name, "contigs.pileup")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/assembly_check__pileup_on_mapped_reads.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/assembly_check__pileup_on_mapped_reads.benchmark")
    shell:
        "pileup.sh threads={threads} -Xmx{resources.memory_in_GB}G in={input.mapped} basecov={output.coverage} out={output.pileup} &> {log}"


rule assembly_check__bin_coverage:
    message:
        "Running step: {rule}"
    input:
        coverage = os.path.join(folder_name, "contigs.cov")
    output:
        contig_depth_yaml = os.path.join(folder_name, "contigs.sum.cov"),
        binned_depth_yaml = os.path.join(folder_name, "contigs.bin.cov")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/assembly_check__bin_coverage.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/assembly_check__bin_coverage.benchmark")
    script:
        os.path.join(os.path.dirname(workflow.snakefile), "../scripts/summarize_depth.py")


rule contaminant_check__declare_contamination:
    message:
        "Running step: {rule}"
    input:
        bracken = os.path.join(folder_name, "bracken.txt")
    output:
        contaminantion_check = os.path.join(folder_name, "contaminantion_check.txt")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/contaminant_check__declare_contamination.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/contaminant_check__declare_contamination.benchmark")
    run:
        with open(output.contaminantion_check, "w") as contaminantion_check:
            df = pandas.read_table(input.bracken)
            if df[df["fraction_total_reads"] > 0.05].shape[0] == 1:
                contaminantion_check.write("No contaminant detected\n")
            else:
                contaminantion_check.write("Contaminant found or Error")


rule species_check__set_species:
    message:
        "Running step: {rule}"
    input:
        bracken = os.path.join(folder_name, "bracken.txt"),
    output:
        species = os.path.join(folder_name, "species.txt")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/contaminant_check__declare_contamination.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/contaminant_check__declare_contamination.benchmark")
    run:
        with open(output.species, "w") as species_file:
            if "species" in config_sample["sample"]:
                species_file.write(config["sample"]["species"] + "\n")
            else:
                df = pandas.read_table(input.bracken)
                species_file.write(df["name"].iloc[0] + "\n")


rule species_check__check_sizes:
    message:
        "Running step: {rule}"
    input:
        contig_depth_yaml = os.path.join(folder_name, "contigs.sum.cov"),
        species = os.path.join(folder_name, "species.txt")
    output:
        size_check = os.path.join(folder_name, "size_check.txt")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/contaminant_check__declare_contamination.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/contaminant_check__declare_contamination.benchmark")
    run:
        with open(output.size_check, "w") as species_file:
            pass
