import re
import pandas
# from pytools.persistent_dict import PersistentDict
# size estimation from kmers as well with jellyfish?
# storage = PersistentDict("qcquickie_storage")
# alternative is to do a quick qc on basepairs followed by a more rigorous assembly

configfile: os.path.join(os.path.dirname(workflow.snakefile), "config/config.yaml")
# requires --config R1_reads={read_location},R2_reads={read_location}
sample = "Test"
R1 = config["R1_reads"],
R2 = config["R2_reads"],

# my understanding is all helps specify final output
onsuccess:
    print("Workflow complete")
    output = ["status.txt"]
    with open(output[0], "w") as status:
        status.write("Success")
onerror:
    print("Workflow error")
    output = ["status.txt"]
    with open(output[0], "w") as status:
        status.write("Failure")

rule all:
    input:
        # analysis/rule run__Kraken_on_reads
        # analysis/set_species_file
        # analysis/run__Ariba_MLST_on_reads
        # analysis/run__Ariba_resfinder_on_reads
        # summarize/script__summarize_kraken_report
        "contigs.sketch"


rule run__bbduk_adapter_trimming_and_filtering:
    message:
        "Denovo assembly of normalized reads"
    input:
        reads = (R1, R2)
    output:
        contigs = "contigs.sketch"
    params:
        options = "".join(config["tadpole"]["options"]),
        adapter = "/tools/git.repositories/SerumQC-private/DB/adapters.fasta"
    log:
        "log/tadpole.log"
    shell:
        """
        bbduk.sh in={input.reads[0]} in2={input.reads[1]} out=filtered_adaptertrimmed.fq ref={params.adapter} ktrim=r k=23 mink=11 hdist=1 tbo minavgquality=14 &>> {log}
        bbmerge.sh in=filtered.fq out=merged.fq outu=unmerged.fq &>> {log}
        tadpole.sh in=merged_unmerged.fq out=contigs.fa &>> {log}
        cat merged.fq unmerged.fq > merged_unmerged.fq
        bbmap.sh ref=contigs.fa in=merged_unmerged.fq out=contigs.sam &>> {log}
        pileup.sh in=contigs.sam out=contigs_min.pileup covwindow=1 covwindowavg=10 overwrite=true &>> {log}
        pileup.sh in=contigs.sam out=contigs_rec.pileup covwindow=1 covwindowavg=25 overwrite=true &>> {log}
        awk -F $'\t' '\{sum += $12\} END \{print sum\}' contigs_min.pileup &>> {log}
        awk -F $'\t' '\{sum += $12\} END \{print sum\}' contigs_rec.pileup &>> {log}
        filterbycoverage.sh in=contigs.fa cov=contigs.pileup minc=10 &>> {log}
        filterbycoverage.sh in=contigs.fa cov=contigs.pileup minc=25 &>> {log}
        sketch.pl in=contigs.fa out=contigs.sketch &>> {log}
        """

